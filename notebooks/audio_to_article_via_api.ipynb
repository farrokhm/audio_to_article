{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ccc2b4",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Audio to Article Notebook\n",
    "This notebook lets you:\n",
    "1. Upload an audio file\n",
    "2. Transcribe speech to text (using Whisper)\n",
    "3. Draft an article from the transcript (via API)\n",
    "4. Improve the article quality (summarization)\n",
    "5. Save the article as *DOCX* and *PDF* files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d411886",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e6c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/openai/whisper.git torch python-docx reportlab google-genai ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d612061b",
   "metadata": {},
   "source": [
    "## üì• Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6c4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import whisper\n",
    "from docx import Document\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from IPython.display import HTML, display\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.platypus import Paragraph, SimpleDocTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ad0ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üéß Step 1: Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9807de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transcript is already extracted and saved.\n",
      "The transcript is loaded and ready.\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio(audio_path: str) -> str:\n",
    "    \"\"\"Transcribe an audio file to text using the Whisper model.\n",
    "\n",
    "    Args:\n",
    "        audio_path: Path to the input audio file.\n",
    "\n",
    "    Returns:\n",
    "        The transcription as a single string.\n",
    "    \"\"\"\n",
    "    model = whisper.load_model(\"small\")  # choose: tiny, base, small, medium, large\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "def ensure_directories(base_path: str, subdirs=None) -> None:\n",
    "    \"\"\"Ensure required subdirectories exist under base_path.\"\"\"\n",
    "    if subdirs is None:\n",
    "        subdirs = (\"audio_files\", \"transcripts\", \"articles\")\n",
    "    for sub in subdirs:\n",
    "        os.makedirs(os.path.join(base_path, sub), exist_ok=True)\n",
    "\n",
    "project_path = (\n",
    "    \"D:\\\\Education_Profession\\\\Projects\\\\Personal_Projects\\\\audio_to_article\\\\\"\n",
    ")\n",
    "\n",
    "# create required folders if missing\n",
    "data_path = project_path + \"data\\\\\"\n",
    "ensure_directories(data_path)\n",
    "\n",
    "file_name = \"AI_Foundation_Model_Selection.mp3\"\n",
    "audio_file_path = project_path + \"data\\\\audio_files\\\\\" + file_name\n",
    "\n",
    "file_name_without_extension, file_extension = os.path.splitext(file_name)\n",
    "transcript_path = (\n",
    "    project_path + \"data\\\\transcripts\\\\\" + file_name_without_extension + \"_transcript.txt\"\n",
    ")\n",
    "if os.path.isfile(transcript_path):\n",
    "    print(\"The transcript is already extracted and saved.\")\n",
    "    # load the transcript (optional)\n",
    "    with open(transcript_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        transcript = file.read()\n",
    "    print(\"The transcript is loaded and ready.\")\n",
    "else:\n",
    "    transcript = transcribe_audio(audio_file_path)\n",
    "    # save the transcript (optional)\n",
    "    with open(transcript_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(transcript)\n",
    "    print(f\"Transcription is extracted and saved to {transcript_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a46feb",
   "metadata": {},
   "source": [
    "### Review the Trascription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62021667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"\n",
       "    max-height: 300px;\n",
       "    overflow-y: scroll;\n",
       "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
       "    padding: 10px;\n",
       "    color: var(--vscode-editor-foreground, #000000);\n",
       "    background-color: var(--vscode-editor-background, #ffffff);\n",
       "\">\n",
       "     If you have a use case for generative AI, how do you decide on which foundation model to pick to run it? With the huge number of foundation models out there, it's not an easy question. Different models are trained on different data and have different parameter counts. And picking the wrong model can have severe unwanted impacts like biases originating from the training data or hallucinations that are just plain wrong. Now, one approach is to just pick the largest, most massive model out there to execute every task. The largest models have huge parameter counts and are usually pretty good generalists, but with large models come costs. Costs of compute, cost of complexity, and costs of variability. So often the better approach is to pick the right size model for the specific use case you have. So let me propose to you an AI model selection framework. It has six pretty simple stages. Let's take a look at what they are, and then give some examples of how this might work. Now, stage one, that is to clearly articulate your use case. What exactly are you planning to use generative AI for? From there, you'll list some of the model options available to you. Perhaps there are already a subset of foundation models running that you have access to. With a short list of models, you'll next want to identify each model's size, performance, costs, risks, and deployment methods. Next, evaluate those model characteristics for your specific use case. Run some tests, that's the next stage. Testing options based on your previously identified use case and deployment needs. And then finally, choose the option that provides the most value. So let's put this framework to the test. Now, my use case, we're going to say that is a use case for text generation. I need the AI to write personalised emails for my awesome marketing campaign. That's stage one. Now, my organisation is already using two foundation models for other things. So I'll evaluate those. First of all, we've got Llama 2 and specifically the Llama 270 model. That's a fairly large model, 70 billion parameters. It's from Meta. And I know it's quite good at some text generation use cases. Then there's also Granite that we have deployed. Granite is a smaller general purpose model, and that's from IBM. And I know there is a 13 billion parameter model that I've heard does quite well with text generation as well. So those are the models I'm going to evaluate Llama 2 and Granite. Next, we need to evaluate model size, performance and risks. And a good place to start here is with the model card. The model card might tell us if the model has been trained on data specifically for our purposes. Pre-trained foundation models are fine-tuned for specific use cases such as sentiment analysis or document summarisation or maybe text generation. And that's important to know because if a model is pre-trained on a use case close to ours, it may perform better when processing our prompts and enable us to use zero-shot prompting to obtain our desired results. And that means we can simply ask the model to perform tasks without having to provide multiple completed examples first. Now, when it comes to evaluating model performance for our use case, we can consider three factors. The first factor that we would consider is accuracy. Now, accuracy denotes how close the generated output is to the desired output. And it can be measured objectively and repeatedly by choosing evaluation metrics that are relevant to your use cases. So for example, if your use case related to text translation, the BLEU, that's the bilingual evaluation under study benchmark, can be used to indicate the quality of the generated translations. Now, the second factor relates to reliability of the model. Now, that's a function of several factors actually, such as consistency, explainability, and trustworthiness, as well as how well a model avoids toxicity like hate speech. Reliability comes down to trust, and trust is built through transparency and traceability of the training data and accuracy and reliability of the output. And then the third factor, that is speed. And specifically we're saying how quickly does the user get a response to a submitted prompt. Now, speed and accuracy are often a trade-off here. Larger models may be slower, but perhaps deliver a more accurate answer. Or then again, maybe the smaller model is faster and has minimal differences in accuracy to the larger model. It really comes down to finding the sweet spot between performance, speed, and cost. A smaller, less expensive model may not offer performance or accuracy metrics on par with an expensive one, but it would still be preferable over the latter if you consider any additional benefits the model might deliver, lower latency, and greater transparency into the model inputs and outputs. The way to find out is to simply select the model that's likely to deliver the desired output and, well, test it. Test that model with your prompts to see if it works, and then assess the model performance and quality of the output using metrics. Now, I've mentioned deployment in passing, so a quick word on that as a decision factor. We need to evaluate where and how we want the model and data to be deployed. So let's say that we're leaning towards Lama 2 as our chosen model based on our testing. Right, cool. Lama 2, that's an open source model, and we could inference with it on a public cloud. So we've got a public cloud already out here. It's got our LLM of choice in it, which is Lama 2. We could just inference to that. But if we decide we want to fine tune the model with our own enterprise data, we might need to deploy it on prem. So this is where we have our own version of Lama 2, and we are going to provide fine tuning to it. Now, deploying on premise gives you greater control and more security benefits compared to a public cloud environment. But it's an expensive proposition, especially when factoring model size and compute power, including the number of GPUs it takes to run a single large language model. Now, everything we've discussed here is tied to a specific use case. But of course, it's quite likely that any given organization will have multiple use cases. And as we run through this AI model selection framework, we might find that each use case is better suited to a different foundation model. That's called a multi-model approach. Essentially, not all AI models are the same, and neither are your use cases. And this framework might be just what you need to pair the models and the use cases together to find a winning combination of both.\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verifying the draft\n",
    "\n",
    "# This CSS uses VS Code's theme variables for colors,\n",
    "# with fallback values for other environments like standard Jupyter.\n",
    "html_code = f\"\"\"\n",
    "<div style=\"\n",
    "    max-height: 300px;\n",
    "    overflow-y: scroll;\n",
    "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
    "    padding: 10px;\n",
    "    color: var(--vscode-editor-foreground, #000000);\n",
    "    background-color: var(--vscode-editor-background, #ffffff);\n",
    "\">\n",
    "    {transcript}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d8d3b",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Step 2: Draft Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9964622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Make sure it is set in your .env file.\")\n",
    "client = genai.Client(api_key=google_api_key)\n",
    "\n",
    "\n",
    "def draft_article(transcript: str, foundation_model: str, temperature: float = 0.5):\n",
    "    \"\"\"Generate an article draft from a transcript using Google GenAI.\n",
    "\n",
    "    Args:\n",
    "        transcript: The transcript text to base the article on.\n",
    "        foundation_model: Model identifier to use for generation (e.g. \"gemini-2.5-flash\").\n",
    "        temperature: Sampling temperature for generation (0.0 - 1.0).\n",
    "\n",
    "    Returns:\n",
    "        Generated article text on success, or None on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article_draft = client.models.generate_content(\n",
    "            model=foundation_model,\n",
    "            config=types.GenerateContentConfig(temperature=temperature),\n",
    "            contents=f\"\"\"Draft an informative, well-structured article with three to five paragraphs based on the following transcript: {transcript}\"\"\",\n",
    "        )\n",
    "        article_draft_text = article_draft.text\n",
    "        return article_draft_text\n",
    "\n",
    "    except Exception as e:\n",
    "        # The function will now handle errors gracefully instead of crashing.\n",
    "        print(f\"An error occurred during article generation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef9163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_draft_text = draft_article(\n",
    "    transcript=transcript, foundation_model=\"gemini-2.5-flash\", temperature=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6662f",
   "metadata": {},
   "source": [
    "### Review the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d858d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"\n",
       "    max-height: 300px;\n",
       "    overflow-y: scroll;\n",
       "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
       "    padding: 10px;\n",
       "    color: var(--vscode-editor-foreground, #000000);\n",
       "    background-color: var(--vscode-editor-background, #ffffff);\n",
       "\">\n",
       "    ## Navigating the Generative AI Landscape: A Framework for Foundation Model Selection\n",
       "\n",
       "Selecting the right foundation model for a generative AI use case is a complex challenge in today's rapidly evolving technological landscape. With a vast array of models differing in training data, parameter counts, and inherent risks like biases or hallucinations, simply choosing the largest model isn't always optimal. While massive models often serve as good generalists, they incur significant costs in compute, complexity, and variability. A more strategic approach involves selecting a \"right-sized\" model tailored to the specific application, necessitating a structured AI model selection framework to guide decision-making and mitigate potential pitfalls.\n",
       "\n",
       "This proposed framework comprises six straightforward stages designed to streamline the model selection process. First, clearly articulate your use case ‚Äì what exactly is the generative AI intended for? Next, list available model options, potentially including those already deployed within your organization. The third stage involves identifying each model's key characteristics: size, performance, costs, risks, and deployment methods. Subsequently, evaluate these characteristics against your specific use case, followed by running targeted tests based on identified needs and deployment requirements. The final stage is to choose the option that delivers the most value, as exemplified by a scenario requiring personalized email generation, where existing models like Meta's Llama 2 (70 billion parameters) and IBM's Granite (13 billion parameters) might be considered for evaluation.\n",
       "\n",
       "Evaluating model characteristics delves deeper into performance, often starting with a model card to understand pre-training and fine-tuning for specific tasks like text generation. Key performance factors include **Accuracy**, measuring how closely the output matches the desired result (e.g., using the BLEU benchmark for text translation). **Reliability** encompasses consistency, explainability, trustworthiness, and the avoidance of toxicity, built on transparency of training data and output. **Speed**, or prompt response time, is the third factor. Crucially, there's often a trade-off between speed and accuracy, and between overall performance and cost. A smaller, less expensive model might be preferable if its accuracy differences are minimal and it offers benefits like lower latency or greater transparency. Testing with specific prompts and evaluating metrics is essential to find this \"sweet spot.\" Deployment is another critical factor, weighing the control and security of on-premise deployment (expensive, high compute demands) against the flexibility of public cloud inference.\n",
       "\n",
       "Ultimately, this selection process is highly dependent on the individual use case. Organizations often have multiple generative AI applications, suggesting that a \"multi-model approach\" ‚Äì where different use cases are best served by different foundation models ‚Äì is frequently the most effective strategy. By systematically applying this framework, businesses can confidently pair the optimal AI models with their specific needs, unlocking maximum value and avoiding costly missteps in their generative AI initiatives.\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_code = f\"\"\"\n",
    "<div style=\"\n",
    "    max-height: 300px;\n",
    "    overflow-y: scroll;\n",
    "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
    "    padding: 10px;\n",
    "    color: var(--vscode-editor-foreground, #000000);\n",
    "    background-color: var(--vscode-editor-background, #ffffff);\n",
    "\">\n",
    "    {article_draft_text}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740c0f9",
   "metadata": {},
   "source": [
    "## üìù Step 3: Improve Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abee6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_article = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=f\"Improve the following article for grammar, clarity, and readability without shortening it. \\\n",
    "        Keep the original title and all section headings exactly as they appear. \\\n",
    "        Return only the improved article text ‚Äî no introductions, explanations, or additional commentary:\\n\\n{article_draft_text}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a495e",
   "metadata": {},
   "source": [
    "### Review the improved article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db52e599",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"\n",
       "    max-height: 300px;\n",
       "    overflow-y: scroll;\n",
       "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
       "    padding: 10px;\n",
       "    color: var(--vscode-editor-foreground, #000000);\n",
       "    background-color: var(--vscode-editor-background, #ffffff);\n",
       "\">\n",
       "    ## Navigating the Generative AI Landscape: A Framework for Foundation Model Selection\n",
       "\n",
       "Selecting the right foundation model for a generative AI use case is a complex challenge in today's rapidly evolving technological landscape. With a vast array of models differing in training data, parameter counts, and inherent risks like biases or hallucinations, simply choosing the largest model isn't always the optimal solution. While massive models often serve as good generalists, they incur significant costs in compute resources, complexity, and variability. A more strategic approach involves selecting a \"right-sized\" model tailored to the specific application, necessitating a structured AI model selection framework to guide decision-making and mitigate potential pitfalls.\n",
       "\n",
       "This proposed framework comprises six straightforward stages designed to streamline the model selection process. First, clearly articulate your use case ‚Äì what exactly is the generative AI intended for? Next, list available model options, potentially including those already deployed within your organization. The third stage involves identifying each model's key characteristics: size, performance, costs, risks, and deployment methods. Subsequently, evaluate these characteristics against your specific use case, followed by running targeted tests based on identified needs and deployment requirements. The final stage is to choose the option that delivers the most value, as exemplified by a scenario requiring personalized email generation, where existing models like Meta's Llama 2 (70 billion parameters) and IBM's Granite (13 billion parameters) might be considered for evaluation.\n",
       "\n",
       "Evaluating model characteristics delves deeper into performance, often starting with a model card to understand pre-training and fine-tuning processes for specific tasks like text generation. Key performance factors include **Accuracy**, measuring how closely the output matches the desired result (e.g., using the BLEU benchmark for text translation). **Reliability** encompasses consistency, explainability, trustworthiness, and the avoidance of toxicity, built on transparency of training data and output. **Speed**, or prompt response time, is the third factor. Crucially, there's often a trade-off between speed and accuracy, and between overall performance and cost. A smaller, less expensive model might be preferable if its accuracy differences are minimal and it offers benefits like lower latency or greater transparency. Testing with specific prompts and evaluating relevant metrics is essential to find this \"sweet spot.\" Deployment is another critical factor, weighing the control and security of on-premise deployment (which can be expensive and demand high compute resources) against the flexibility of public cloud inference.\n",
       "\n",
       "Ultimately, this selection process is highly dependent on the individual use case. Organizations often have multiple generative AI applications, suggesting that a \"multi-model approach\" ‚Äì where different use cases are best served by different foundation models ‚Äì is frequently the most effective strategy. By systematically applying this framework, businesses can confidently pair the optimal AI models with their specific needs, unlocking maximum value and avoiding costly missteps in their generative AI initiatives.\n",
       "\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "improved_article_text = improved_article.text\n",
    "html_code = f\"\"\"\n",
    "<div style=\"\n",
    "    max-height: 300px;\n",
    "    overflow-y: scroll;\n",
    "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
    "    padding: 10px;\n",
    "    color: var(--vscode-editor-foreground, #000000);\n",
    "    background-color: var(--vscode-editor-background, #ffffff);\n",
    "\">\n",
    "    {improved_article_text}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c60d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üíæ Step 4: Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6551cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_docx(text: str, filename: str):\n",
    "    \"\"\"Write plain text into a .docx file.\n",
    "\n",
    "    Args:\n",
    "        text: Text content to write into the document.\n",
    "        filename: Destination .docx file path.\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    doc.add_paragraph(text)\n",
    "    doc.save(filename)\n",
    "\n",
    "\n",
    "def export_to_pdf(text: str, filename: str):\n",
    "    \"\"\"Export text to a PDF using ReportLab Platypus Paragraphs.\n",
    "\n",
    "    Produces a simple single-flow PDF that wraps text correctly.\n",
    "\n",
    "    Args:\n",
    "        text: Text content to export.\n",
    "        filename: Destination .pdf file path.\n",
    "    \"\"\"\n",
    "    doc = SimpleDocTemplate(filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    formatted_text = text.replace(\"\\n\", \"<br/>\")\n",
    "    p = Paragraph(formatted_text, styles[\"Normal\"])\n",
    "    doc.build([p])\n",
    "\n",
    "\n",
    "save_directory = project_path + \"data\\\\articles\\\\\"\n",
    "file_name = file_name_without_extension + \"_article\"\n",
    "save_article_path = os.path.join(save_directory, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38705236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Article is saved correctly to: D:\\Education_Profession\\Projects\\Personal_Projects\\audio_to_article\\data\\articles\\AI_Foundation_Model_Selection_article as a MS Word file.\n"
     ]
    }
   ],
   "source": [
    "# Export the article as a MS Word file\n",
    "export_to_docx(improved_article_text, save_article_path + \".docx\")\n",
    "print(f\"The Article is saved correctly to: {save_article_path} as a MS Word file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2be72892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article is saved correctly to: D:\\Education_Profession\\Projects\\Personal_Projects\\audio_to_article\\data\\articles\\AI_Foundation_Model_Selection_article as a PDF file.\n"
     ]
    }
   ],
   "source": [
    "export_to_pdf(improved_article_text, save_article_path + \".pdf\")\n",
    "print(f\"The article is saved correctly to: {save_article_path} as a PDF file.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "personal-projects-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
