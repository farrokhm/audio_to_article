{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af006683",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Audio to Article Notebook\n",
    "This notebook lets you:\n",
    "1. Upload an audio file,\n",
    "2. Transcribe speech to text (using Whisper),\n",
    "3. Draft an article from the transcript (via API),\n",
    "4. Improve the article quality (summarization),\n",
    "5. Export the article to **DOCX** and **PDF**,\n",
    "6. Download the exported files directly inside the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7219f9e9",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/openai/whisper.git torch python-docx reportlab google-genai ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ae6dd",
   "metadata": {},
   "source": [
    "## üì• Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3632c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import whisper\n",
    "from docx import Document\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from IPython.display import HTML, display\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.platypus import Paragraph, SimpleDocTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efcbca9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üéß Step 1: Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49c105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Education_Profession\\Projects\\Personal_Projects\\audio_to_article\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription is extracted and saved to D:\\Education_Profession\\Projects\\Personal_Projects\\audio_to_article\\\\transcripts\\AI_Foundation_Model_Selection_transcript.txt\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio(audio_path: str) -> str:\n",
    "    model = whisper.load_model(\"small\")  # choose: tiny, base, small, medium, large\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "project_path = (\n",
    "    \"D:\\\\Education_Profession\\\\Projects\\\\Personal_Projects\\\\audio_to_article\\\\\"\n",
    ")\n",
    "file_name = \"AI_Foundation_Model_Selection.mp3\"\n",
    "audio_file_path = project_path + \"audio_files\\\\\" + file_name\n",
    "\n",
    "file_name_without_extension, file_extension = os.path.splitext(file_name)\n",
    "transcript_path = (\n",
    "    project_path + \"\\\\transcripts\\\\\" + file_name_without_extension + \"_transcript.txt\"\n",
    ")\n",
    "if os.path.isfile(transcript_path):\n",
    "    print(\"The transcript is already extracted and saved.\")\n",
    "    # load the transcript (optional)\n",
    "    with open(transcript_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        transcript = file.read()\n",
    "    print(\"The transcript is loaded and ready.\")\n",
    "else:\n",
    "    transcript = transcribe_audio(audio_file_path)\n",
    "    # save the transcript (optional)\n",
    "    with open(transcript_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(transcript)\n",
    "    print(f\"Transcription is extracted and saved to {transcript_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324ed0a",
   "metadata": {},
   "source": [
    "### Review the Trascription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"\n",
       "    max-height: 300px;\n",
       "    overflow-y: scroll;\n",
       "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
       "    padding: 10px;\n",
       "    color: var(--vscode-editor-foreground, #000000);\n",
       "    background-color: var(--vscode-editor-background, #ffffff);\n",
       "\">\n",
       "     If you have a use case for generative AI, how do you decide on which foundation model to pick to run it? With the huge number of foundation models out there, it's not an easy question. Different models are trained on different data and have different parameter counts. And picking the wrong model can have severe unwanted impacts like biases originating from the training data or hallucinations that are just plain wrong. Now, one approach is to just pick the largest, most massive model out there to execute every task. The largest models have huge parameter counts and are usually pretty good generalists, but with large models come costs. Costs of compute, cost of complexity, and costs of variability. So often the better approach is to pick the right size model for the specific use case you have. So let me propose to you an AI model selection framework. It has six pretty simple stages. Let's take a look at what they are, and then give some examples of how this might work. Now, stage one, that is to clearly articulate your use case. What exactly are you planning to use generative AI for? From there, you'll list some of the model options available to you. Perhaps there are already a subset of foundation models running that you have access to. With a short list of models, you'll next want to identify each model's size, performance, costs, risks, and deployment methods. Next, evaluate those model characteristics for your specific use case. Run some tests, that's the next stage. Testing options based on your previously identified use case and deployment needs. And then finally, choose the option that provides the most value. So let's put this framework to the test. Now, my use case, we're going to say that is a use case for text generation. I need the AI to write personalised emails for my awesome marketing campaign. That's stage one. Now, my organisation is already using two foundation models for other things. So I'll evaluate those. First of all, we've got Llama 2 and specifically the Llama 270 model. That's a fairly large model, 70 billion parameters. It's from Meta. And I know it's quite good at some text generation use cases. Then there's also Granite that we have deployed. Granite is a smaller general purpose model, and that's from IBM. And I know there is a 13 billion parameter model that I've heard does quite well with text generation as well. So those are the models I'm going to evaluate Llama 2 and Granite. Next, we need to evaluate model size, performance and risks. And a good place to start here is with the model card. The model card might tell us if the model has been trained on data specifically for our purposes. Pre-trained foundation models are fine-tuned for specific use cases such as sentiment analysis or document summarisation or maybe text generation. And that's important to know because if a model is pre-trained on a use case close to ours, it may perform better when processing our prompts and enable us to use zero-shot prompting to obtain our desired results. And that means we can simply ask the model to perform tasks without having to provide multiple completed examples first. Now, when it comes to evaluating model performance for our use case, we can consider three factors. The first factor that we would consider is accuracy. Now, accuracy denotes how close the generated output is to the desired output. And it can be measured objectively and repeatedly by choosing evaluation metrics that are relevant to your use cases. So for example, if your use case related to text translation, the BLEU, that's the bilingual evaluation under study benchmark, can be used to indicate the quality of the generated translations. Now, the second factor relates to reliability of the model. Now, that's a function of several factors actually, such as consistency, explainability, and trustworthiness, as well as how well a model avoids toxicity like hate speech. Reliability comes down to trust, and trust is built through transparency and traceability of the training data and accuracy and reliability of the output. And then the third factor, that is speed. And specifically we're saying how quickly does the user get a response to a submitted prompt. Now, speed and accuracy are often a trade-off here. Larger models may be slower, but perhaps deliver a more accurate answer. Or then again, maybe the smaller model is faster and has minimal differences in accuracy to the larger model. It really comes down to finding the sweet spot between performance, speed, and cost. A smaller, less expensive model may not offer performance or accuracy metrics on par with an expensive one, but it would still be preferable over the latter if you consider any additional benefits the model might deliver, lower latency, and greater transparency into the model inputs and outputs. The way to find out is to simply select the model that's likely to deliver the desired output and, well, test it. Test that model with your prompts to see if it works, and then assess the model performance and quality of the output using metrics. Now, I've mentioned deployment in passing, so a quick word on that as a decision factor. We need to evaluate where and how we want the model and data to be deployed. So let's say that we're leaning towards Lama 2 as our chosen model based on our testing. Right, cool. Lama 2, that's an open source model, and we could inference with it on a public cloud. So we've got a public cloud already out here. It's got our LLM of choice in it, which is Lama 2. We could just inference to that. But if we decide we want to fine tune the model with our own enterprise data, we might need to deploy it on prem. So this is where we have our own version of Lama 2, and we are going to provide fine tuning to it. Now, deploying on premise gives you greater control and more security benefits compared to a public cloud environment. But it's an expensive proposition, especially when factoring model size and compute power, including the number of GPUs it takes to run a single large language model. Now, everything we've discussed here is tied to a specific use case. But of course, it's quite likely that any given organization will have multiple use cases. And as we run through this AI model selection framework, we might find that each use case is better suited to a different foundation model. That's called a multi-model approach. Essentially, not all AI models are the same, and neither are your use cases. And this framework might be just what you need to pair the models and the use cases together to find a winning combination of both.\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verifying the draft\n",
    "\n",
    "# This CSS uses VS Code's theme variables for colors,\n",
    "# with fallback values for other environments like standard Jupyter.\n",
    "html_code = f\"\"\"\n",
    "<div style=\"\n",
    "    max-height: 300px;\n",
    "    overflow-y: scroll;\n",
    "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
    "    padding: 10px;\n",
    "    color: var(--vscode-editor-foreground, #000000);\n",
    "    background-color: var(--vscode-editor-background, #ffffff);\n",
    "\">\n",
    "    {transcript}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eecb86",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Step 2: Draft Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=google_api_key)\n",
    "\n",
    "\n",
    "def draft_article(transcript: str, foundation_model: str, temperature: float = 0.5):\n",
    "    try:\n",
    "        article_draft = client.models.generate_content(\n",
    "            model=foundation_model,\n",
    "            config=types.GenerateContentConfig(temperature=temperature),\n",
    "            contents=f\"\"\"Draft an informative, well-structured article with three to five paragraphs based on the following transcript: {transcript}\"\"\",\n",
    "        )\n",
    "        article_draft_text = article_draft.text\n",
    "        return article_draft_text\n",
    "\n",
    "    except Exception as e:\n",
    "        # The function will now handle errors gracefully instead of crashing.\n",
    "        print(f\"An error occurred during article generation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_draft_text = draft_article(\n",
    "    transcript=transcript, foundation_model=\"gemini-2.5-flash\", temperature=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba87a9c",
   "metadata": {},
   "source": [
    "### Review the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff86289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"\n",
       "    max-height: 300px;\n",
       "    overflow-y: scroll;\n",
       "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
       "    padding: 10px;\n",
       "    color: var(--vscode-editor-foreground, #000000);\n",
       "    background-color: var(--vscode-editor-background, #ffffff);\n",
       "\">\n",
       "    ## Navigating the Generative AI Landscape: A Strategic Model Selection Framework\n",
       "\n",
       "The rapid proliferation of generative AI foundation models presents a significant challenge for organizations seeking to implement this transformative technology. With diverse training data, varying parameter counts, and distinct capabilities, selecting the appropriate model is crucial, as an incorrect choice can lead to severe issues like biased outputs originating from training data or outright hallucinations. While the instinct might be to opt for the largest, most generalist model available, this often incurs substantial costs in compute, complexity, and variability. A more strategic approach involves identifying the right-sized model for a specific use case, necessitating a structured decision-making process. This article outlines a comprehensive AI model selection framework designed to guide organizations through this complex landscape.\n",
       "\n",
       "The proposed framework consists of six distinct stages for strategic model selection. It begins with clearly articulating the specific generative AI use case, followed by listing available model options, potentially starting with those already in use within the organization. The third stage involves identifying each model's key characteristics, including size, performance, costs, risks, and deployment methods. These characteristics are then rigorously evaluated against the specific use case requirements. The penultimate stage is crucial: running targeted tests with the identified options. Finally, the organization chooses the model that delivers the most value. For instance, if the use case is generating personalized marketing emails, an organization might evaluate existing models like Meta's Llama 2 (70 billion parameters) and IBM's Granite (13 billion parameters) based on their known suitability for text generation.\n",
       "\n",
       "Deeper evaluation of potential models involves scrutinizing model cards to understand their pre-training and fine-tuning, which can indicate suitability for specific tasks and enable efficient zero-shot prompting. Key performance indicators include accuracy, measured objectively through relevant metrics (e.g., BLEU for text translation), and reliability, encompassing consistency, explainability, trustworthiness, and the avoidance of toxic outputs. Speed, or the prompt response time, is the third critical factor. Organizations must navigate the inherent trade-offs, as larger models may offer higher accuracy but slower speeds and greater costs, while smaller models might be faster with acceptable accuracy. The optimal choice often lies in finding the \"sweet spot\" that balances performance, speed, and cost, alongside other benefits like lower latency or greater transparency. Finally, deployment considerations, such as public cloud inference versus more controlled but expensive on-premise fine-tuning, significantly influence the final decision.\n",
       "\n",
       "Ultimately, the selection process culminates in rigorous testing of promising models against the defined use case, using specific prompts and evaluating output quality with established metrics. It's important to recognize that a single organization will likely have multiple generative AI use cases, and the framework often reveals that a \"multi-model approach\" is most effective, where different foundation models are best suited for different tasks. By systematically applying this AI model selection framework, organizations can move beyond guesswork, strategically pairing the right models with their unique use cases to achieve optimal performance, efficiency, and business value.\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_code = f\"\"\"\n",
    "<div style=\"\n",
    "    max-height: 300px;\n",
    "    overflow-y: scroll;\n",
    "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
    "    padding: 10px;\n",
    "    color: var(--vscode-editor-foreground, #000000);\n",
    "    background-color: var(--vscode-editor-background, #ffffff);\n",
    "\">\n",
    "    {article_draft_text}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc601a7",
   "metadata": {},
   "source": [
    "## üìù Step 3: Improve Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74173c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_article = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=f\"Improve the following article for grammar, clarity, and readability without shortening it. \\\n",
    "        Keep the original title and all section headings exactly as they appear. \\\n",
    "        Return only the improved article text ‚Äî no introductions, explanations, or additional commentary:\\n\\n{article_draft_text}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72a909",
   "metadata": {},
   "source": [
    "### Review the improved article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b369f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"\n",
       "    max-height: 300px;\n",
       "    overflow-y: scroll;\n",
       "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
       "    padding: 10px;\n",
       "    color: var(--vscode-editor-foreground, #000000);\n",
       "    background-color: var(--vscode-editor-background, #ffffff);\n",
       "\">\n",
       "    ## Navigating the Generative AI Landscape: A Strategic Model Selection Framework\n",
       "\n",
       "The rapid proliferation of generative AI foundation models presents a significant challenge for organizations seeking to implement this transformative technology. With diverse training data, varying parameter counts, and distinct capabilities, selecting the appropriate model is crucial; an incorrect choice can lead to serious issues, such as biased outputs originating from the training data or outright hallucinations. While the initial instinct might be to opt for the largest, most generalist model available, this often incurs substantial costs in compute resources, complexity, and output variability. A more strategic approach involves identifying the right-sized model for a specific use case, necessitating a structured decision-making process. This article outlines a comprehensive AI model selection framework designed to guide organizations through this complex landscape.\n",
       "\n",
       "The proposed framework consists of six distinct stages for strategic model selection. It begins with clearly articulating the specific generative AI use case, followed by listing available model options, potentially starting with those already in use within the organization. The third stage involves identifying each model's key characteristics, including size, performance, costs, risks, and deployment methods. These characteristics are then rigorously evaluated against the specific use case requirements. The penultimate stage is crucial: running targeted tests with the identified options. Finally, the organization chooses the model that delivers the most value. For instance, if the use case is generating personalized marketing emails, an organization might evaluate existing models like Meta's Llama 2 (70 billion parameters) and IBM's Granite (13 billion parameters) based on their known suitability for text generation.\n",
       "\n",
       "Deeper evaluation of potential models involves scrutinizing model cards to understand their pre-training and fine-tuning, which can indicate suitability for specific tasks and enable efficient zero-shot prompting. Key performance indicators include accuracy, measured objectively through relevant metrics (e.g., BLEU for text translation), and reliability, encompassing consistency, explainability, trustworthiness, and the avoidance of toxic outputs. Speed, or the prompt response time, is the third critical factor. Organizations must navigate the inherent trade-offs, as larger models may offer higher accuracy but slower speeds and greater costs, while smaller models might be faster with acceptable accuracy. The optimal choice often lies in finding the \"sweet spot\" that balances performance, speed, and cost, alongside other benefits like lower latency or greater transparency. Finally, deployment considerations, such as public cloud inference versus more controlled but expensive on-premise fine-tuning, significantly influence the final decision.\n",
       "\n",
       "Ultimately, the selection process culminates in rigorous testing of promising models against the defined use case, using specific prompts and evaluating output quality with established metrics. It's important to recognize that a single organization will likely have multiple generative AI use cases, and the framework often reveals that a \"multi-model approach\" is most effective, where different foundation models are best suited for different tasks. By systematically applying this AI model selection framework, organizations can move beyond guesswork, strategically pairing the right models with their unique use cases to achieve optimal performance, efficiency, and business value.\n",
       "\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "improved_article_text = improved_article.text\n",
    "html_code = f\"\"\"\n",
    "<div style=\"\n",
    "    max-height: 300px;\n",
    "    overflow-y: scroll;\n",
    "    border: 1px solid var(--vscode-contrastBorder, #e0e0e0);\n",
    "    padding: 10px;\n",
    "    color: var(--vscode-editor-foreground, #000000);\n",
    "    background-color: var(--vscode-editor-background, #ffffff);\n",
    "\">\n",
    "    {improved_article_text}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048c810",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üíæ Step 4: Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_docx(text: str, filename: str):\n",
    "    doc = Document()\n",
    "    doc.add_paragraph(text)\n",
    "    doc.save(filename)\n",
    "\n",
    "\n",
    "def export_to_pdf(text: str, filename: str):\n",
    "    \"\"\"\n",
    "    The corrected function using Platypus Paragraphs for proper text wrapping.\n",
    "    \"\"\"\n",
    "    doc = SimpleDocTemplate(filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    formatted_text = text.replace(\"\\n\", \"<br/>\")\n",
    "    p = Paragraph(formatted_text, styles[\"Normal\"])\n",
    "    doc.build([p])\n",
    "\n",
    "\n",
    "save_directory = project_path + \"articles\\\\\"\n",
    "file_name = file_name_without_extension + \"_article\"\n",
    "save_article_path = os.path.join(save_directory, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e53d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Article is saved correctly to: D:\\Education_Profession\\Projects\\Personal_Projects\\audio_to_article\\articles\\AI_Foundation_Model_Selection_article as a MS Word file.\n"
     ]
    }
   ],
   "source": [
    "# Export the article as a MS Word file\n",
    "export_to_docx(improved_article_text, save_article_path + \".docx\")\n",
    "print(f\"The Article is saved correctly to: {save_article_path} as a MS Word file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a90f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article is saved correctly to: D:\\Education_Profession\\Projects\\Personal_Projects\\audio_to_article\\articles\\AI_Foundation_Model_Selection_article as a PDF file.\n"
     ]
    }
   ],
   "source": [
    "export_to_pdf(improved_article_text, save_article_path + \".pdf\")\n",
    "print(f\"The article is saved correctly to: {save_article_path} as a PDF file.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "personal-projects-py3.13",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
